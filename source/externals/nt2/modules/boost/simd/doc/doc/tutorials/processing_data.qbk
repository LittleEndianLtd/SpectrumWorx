[section Processing data the SIMD way]

[h2 Objectives]
This tutorial will present how data can be processed using Boost.SIMD by writing
a naive `dot` function using Boost.SIMD.

A simple sequential, scalar `dot` function can be simply defined as:

  template<typename Value>
  Value dot(Value* first1, Value* last1, Value* first2)
  {
    Value v(0);

    while(first1 != last1)
    {
      v += *first1 *  *first2;
      first1++;
      first2++;
    }

    return v;
  }

`dot` simply iterates over data pointed by `first1` and `first2`, computes the
product of said data and sum them along.

[h2 Transition from scalar to SIMD code]
If the algorithm is clearly vectorizable, it has to be modified so its SIMD nature
becomes apparent.

First, we arbitrarily unroll the code by an arbitrary factor to make data
parallelism obvious:

  template<typename Value>
  Value dot(Value* first1, Value* last1, Value* first2)
  {
    Value v,v1(0),v2(0);

    // Let's consider that (last1-first1) is divisible by 2
    while(first1 != last1)
    {
      v1 += *first1 *  *first2;
      first1++;
      first2++;

      v2 += *first1 *  *first2;
      first1++;
      first2++;
    }

    v = v1 + v2;

    return v;
  }

The algorithm is split in two parts:

* First, we run over every elements inside both datasets and multiply them.
* Then, we sum the intermediate values into the final result.

By unrolling this pattern to arbitrary size, we expose the fact that the
multiplication between the two dataset is purely "vertical" and so, is
vectorizable. The sum of the partial result itself is a "horizontal" operation,
i.e a vectorizable computation operating across the element of a single vector
(see [link boost_simd_intra_register_operations intra-register operation]).

[h2 Building a SIMD loop nest]
We are now going to use [classref boost::simd::pack] to actually vectorize this
algorithm. The main idea is to computes partial sum inside an instance of
[classref boost::simd::pack] and perform a final summation at the end. To do so,
we will use [funcref boost::simd::load] to load data from `first1` and `first2`,
process those [classref boost::simd::pack] instances using the proper operators
and advance the pointers by the size of the [classref boost::simd::pack].

  #include <boost/simd/sdk/simd/pack.hpp>
  #include <boost/simd/include/functions/sum.hpp>
  #include <boost/simd/include/functions/load.hpp>
  #include <boost/simd/include/functions/plus.hpp>
  #include <boost/simd/include/functions/multiplies.hpp>

  template<typename Value>
  Value dot(Value* first1, Value* last1, Value* first2)
  {
    using boost::simd::sum;
    using boost::simd::pack;
    using boost::simd::load;

    typedef pack<Value> type;
    type tmp;

    // Let's consider that (last1-first1) is divisible by the size of the pack.
    while(first1 != last1)
    {
      // Load current values from the datasets
      pack<Value> x1 = load< type >(first1);
      pack<Value> x2 = load< type >(first2);

      // Computation
      tmp = tmp + x1 * x2;

      // Advance to the next SIMD vector
      first1 += type::static_size;
      first2 += type::static_size;
    }

    return sum(tmp);
  }

That's it. Look at how the computation code looks a lot like the scalar version
and how we simply jump over data using the [classref boost::simd::pack] size.

[h2 Preparing the data]
Now that our SIMD dot product function is ready, we have to apply it on some
data. As currently written, one can simply call `dot` on any piece of memory
of the proper size.

``
  #include <vector>

  int main()
  {
    std::vector<float> a(1024), b(1024);

    // ... fill up a and b

    float r = dot(&a[0], &a[0]+1024, &b[0]);
  }
``

If this version works, the issue is that we don't use aligned load to fill SIMD
register from the memory. On some system, typically pre-Nehalem for x86 or PowerPC,
unaligned loads and stores are far more costlier than aligned one. It's a good
idea, if possible, to be working with aligned data in memory so we can leverage
this faster memory access. To do so, we need to modify the code in two places.

* First the dot function should use the [funcref boost::simd::aligned_load] function
which behaves exactly as [funcref boost::simd::load] but awaits aligned memory as
an input. The code then becomes:

``
    pack<Value> x1 = aligned_load< type >(first1);
    pack<Value> x2 = aligned_load< type >(first2);
``

An alternative is to use the constructor from aligned pointer of [classref boost::simd::pack],
giving us the following code:

``
    pack<Value> x1(first1);
    pack<Value> x2(first2);
``

* Then, we need to provide `dot` pointer to aligned memory. This can be done using
 [classref boost::simd::allocator] as the `std::vector` allocator.

``
    #include <vector>
    #include <boost/simd/memory/allocator.hpp>

    int main()
    {
      std::vector<float, boost::simd::allocator<float> > a(1024), b(1024);

      // ... fill up a and b

      float r = dot(&a[0], &a[0]+1024, &b[0]);
    }
``

The resulting code generation for a SSE2 system is still showing no abstraction
penalty:

          cmp    %rdi,%rsi
          xorps  %xmm1,%xmm1
          je     end
  begin:  movaps (%rdi),%xmm0
          add    $0x10,%rdi
          mulps  (%rdx),%xmm0
          add    $0x10,%rdx
          cmp    %rdi,%rsi
          addps  %xmm0,%xmm1
          jne    begin
  end:    movaps %xmm1,%xmm2
          shufps $0x4e,%xmm1,%xmm2
          addps  %xmm1,%xmm2
          movaps %xmm2,%xmm0
          shufps $0x91,%xmm2,%xmm0
          addps  %xmm2,%xmm0
          retq

This first hand-written version of `dot` still has some shortcomings as it
requires the size of the data to be a multiple of the pack cardinal and doesn't
perform loop unrolling. Both of these changes are left as an exercise to the
reader.

[h2 What to remember ?]
In this tutorial, we used Boost.SIMD components to built a toy dot product
function. By doing we went over:

* the use of [classref boost::simd::pack] as a value-semantic wrapper around SIMD
  register with a proper set of functions;
* the way to go from a sequential to SIMD algorithm;
* how to allocate and load from aligned memory and why it may be important

[endsect]
